{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "\n",
    "import prophet\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from prophet import Prophet\n",
    "import ruptures as rpt\n",
    "from ruptures.utils import pairwise\n",
    "from itertools import cycle\n",
    "\n",
    "from slack.web.client import WebClient\n",
    "from slack.errors import SlackApiError\n",
    "\n",
    "\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (20, 16)\n",
    "mpl.rcParams['axes.grid'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dotenv_path = Path('../.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "password = os.getenv('PASSWORD')\n",
    "user = os.getenv('USER')\n",
    "host = os.getenv('HOST')\n",
    "database = os.getenv('DATABASE')\n",
    "slack_token = os.getenv('SLACK_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine(f'postgresql://{user}:{password}@{host}/{database}')\n",
    "\n",
    "client = WebClient(token=slack_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class AnomalyData:\n",
    "    df : pd.DataFrame\n",
    "    customer : str\n",
    "    P : float\n",
    "    outliers : list\n",
    "    changepoints : list\n",
    "    # extract_date\n",
    "    # ad_params: dict[str, object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#module function\n",
    "\n",
    "def reindex_df(df_inp):\n",
    "    start_date = df_inp.ds.iloc[0] \n",
    "    end_date = df_inp.ds.iloc[-1]\n",
    "    idx = pd.date_range(start_date, end_date)\n",
    "\n",
    "    df = df_inp.set_index('ds')#,inplace=True)\n",
    "    df = df.reindex(idx)\n",
    "    df['y']= df['y'].interpolate(method='linear')\n",
    "    df = df.rename_axis('ds').reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    '''\n",
    "    SELECT a.customer as customer,\n",
    "    DATE(a.startdate) as ds,\n",
    "    COUNT(DISTINCT r.driverid) as y\n",
    "    FROM log_riskscore r\n",
    "    LEFT JOIN log_apicall a on r.apicallid=a.id\n",
    "    AND a.startdate > date :startdate\n",
    "    AND a.startdate <= date :extract_date\n",
    "    WHERE a.route ='score_drivers'\n",
    "    GROUP BY a.customer, DATE(a.startdate)\n",
    "    '''\n",
    "    ]\n",
    "\n",
    "\n",
    "class DataGenerator(object):\n",
    "    def __init__(self,queries, engine, extract_window = 90, **ad_params):\n",
    "        self.queries = queries\n",
    "        self.extract_window = extract_window\n",
    "        self.engine = engine\n",
    "        # self.ad_params = ad_params\n",
    "        # self.datetime_colname = datetime_colname #(ad_param)\n",
    "        # self.y_colname = y_colname # (ad_param)\n",
    "        # possibly add the test and train window lengths so we can filter out dfs with len < test+train window\n",
    "        # return self\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        with self.engine.connect() as con:\n",
    "            for query in self.queries:\n",
    "                sql = text(query)\n",
    "                df = pd.read_sql_query(sql, con, params={'startdate':(pd.Timestamp('today') - pd.Timedelta(days=self.extract_window)).strftime(\"%Y-%m-%d\"), 'extract_date':pd.Timestamp('today').strftime(\"%Y-%m-%d\")})\n",
    "                # This data contains all customers\n",
    "                output = []\n",
    "                customer_names = df.customer.unique()\n",
    "                #create a data frame dictionary to store your data frames\n",
    "                dataframe_dict = {cust : pd.DataFrame() for cust in customer_names}\n",
    "\n",
    "                for key in dataframe_dict.keys():\n",
    "                    dataframe_dict[key] = df.drop('customer', axis=1)[df.customer == key]\n",
    "                    dataframe_dict[key].ds = pd.to_datetime(dataframe_dict[key].ds)\n",
    "                    dataframe_dict[key].reset_index(inplace=True, drop=True)\n",
    "                    ad = AnomalyData(dataframe_dict[key], customer=key, P=0, outliers= list(), changepoints=list())\n",
    "                    output.append(ad)\n",
    "\n",
    "        # the output will contaim certain customers who don't have enough data \n",
    "        # let's retain the dfs that have atleast entries over half the extract window\n",
    "        # aka if we extract for 90 days, lets delete the data that are less than 45 entries \n",
    "        filtered_output = [i for i in output if len(i.df) >= 0.75*(self.extract_window)]\n",
    "\n",
    "        # the index of all these dfs needs to be reset because the train/test indices wont work otherwise\n",
    "        for o in filtered_output:\n",
    "            o.df.reset_index(inplace=True, drop=True)\n",
    "            o.df = reindex_df(o.df)\n",
    "\n",
    "        return filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector:\n",
    "    def __init__(self, data, test_window=14, train_window=21, beta=0.1, \n",
    "    ruptures_changepnt_penalty=10, prophet_interval_width=0.95, \n",
    "    prophet_changepnt_prior = 0.15, weekly_seasonality= False):\n",
    "        '''\n",
    "        Args\n",
    "        ---\n",
    "        changepoint_penalty: float\n",
    "        Higher the value, more conservative the segmentation, must be >0\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.test_window = test_window\n",
    "        self.train_window = train_window\n",
    "        self.beta = beta\n",
    "        self.ruptures_changepnt_penalty = ruptures_changepnt_penalty\n",
    "        self.prophet_interval_width = prophet_interval_width\n",
    "        self.prophet_changepnt_prior = prophet_changepnt_prior\n",
    "        self.weekly_seasonality = weekly_seasonality\n",
    "\n",
    "    def apply(self):\n",
    "        self.apply_changepnt_detection().prophet_fit().get_outliers()#.prophet_plot()\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def apply_changepnt_detection(self,changepnt_model=\"rbf\"):\n",
    "            \n",
    "        '''\n",
    "        Apply changepoint detection ala ruptures package and generate breakpoint indices\n",
    "        Args\n",
    "        ---\n",
    "        changepoint_model: str\n",
    "            Pelt segment model, [\"l1\", \"l2\", \"rbf\"] \n",
    "            https://centre-borelli.github.io/ruptures-docs/code-reference/detection/pelt-reference/\n",
    "\n",
    "        \n",
    "        Returns\n",
    "        ---\n",
    "        breakpoints: list\n",
    "            List of breakpoint indices\n",
    "        '''\n",
    "        signal = self.data.df['y'].to_numpy()\n",
    "        algo = rpt.Pelt(model=changepnt_model).fit(signal)\n",
    "        result = algo.predict(pen=self.ruptures_changepnt_penalty)\n",
    "        breakpoints = [1] + sorted(result)\n",
    "        self.data.changepoints = breakpoints\n",
    "        return self\n",
    "\n",
    "    def plot_changepoints(self): \n",
    "        color_cycle = cycle(self.color_cycle)\n",
    "        fig, ax = plt.subplots(1,1,figsize=(16,8))\n",
    "        ax.plot(self.data.df['ds'],self.data.df['y'], '-o', markersize=2, linewidth=1)\n",
    "        for (start, end), col in zip(pairwise(self.data.breakpoints), color_cycle):\n",
    "            ax.axvspan(self.data.df.iloc[max(0,start-1)]['ds'], self.data.df.iloc[end-1]['ds'], facecolor=col,alpha=0.2)\n",
    "        plt.show()\n",
    "\n",
    "    def prophet_fit(self):\n",
    "\n",
    "        #instantiate the prophet model\n",
    "        prophet_model = Prophet(interval_width=self.prophet_interval_width,\n",
    "            yearly_seasonality=False,\n",
    "            weekly_seasonality=self.weekly_seasonality,\n",
    "            changepoint_prior_scale=self.prophet_changepnt_prior\n",
    "            )\n",
    "        \n",
    "        #segment time frames\n",
    "        # Test/Forecast window\n",
    "        end_index = self.data.df.index[-1]\n",
    "        test_start_index = end_index - self.test_window + 1 \n",
    "        print(f'TEST start index is {test_start_index}')\n",
    "        print(f'TEST END index is {end_index}') #the test end index is setup to be the last index of df\n",
    "        \n",
    "        # Train window, starts at the last changepoint unless the train window goes further back\n",
    "        if test_start_index - self.data.changepoints[-2] < 7:\n",
    "            train_start_index = test_start_index - self.train_window\n",
    "        # elif self.data.changepoints[-2] > 0:\n",
    "        #     train_start_index = self.data.changepoints[-2] - 1\n",
    "        else:\n",
    "            train_start_index = self.data.changepoints[-2] - 1\n",
    "        print(f'TRAIN start index is {train_start_index}')\n",
    "\n",
    "        train_end_index = test_start_index - 1\n",
    "        print(f'TRAIN end index is {train_end_index}')\n",
    "        baseline_ts = self.data.df['ds'][train_start_index:train_end_index+1]\n",
    "        baseline_y = self.data.df['y'][train_start_index:train_end_index+1]\n",
    "        print('TRAIN from {} to {}'.format(self.data.df['ds'][train_start_index], self.data.df['ds'][train_end_index+1]))\n",
    "        print('PREDICT from {} to {}'.format(self.data.df['ds'][test_start_index], self.data.df['ds'][end_index]))\n",
    "\n",
    "        # fit the model\n",
    "        prophet_model.fit(pd.DataFrame({'ds': baseline_ts.values,\n",
    "                                        'y': baseline_y.values}),  algorithm = 'Newton')\n",
    "        \n",
    "        future = prophet_model.make_future_dataframe(periods=self.test_window )\n",
    "        # make prediction\n",
    "        forecast = prophet_model.predict(future)\n",
    "        self.forecast = forecast\n",
    "        self.model = prophet_model\n",
    "        return self\n",
    "\n",
    "    def get_outliers(self):\n",
    "        \"\"\"\n",
    "        Combine the actual values and forecast in a data frame and identify the outliers\n",
    "        Args\n",
    "        ----\n",
    "        df : pandas DataFrame\n",
    "            The daily time-series data set contains ds column for\n",
    "            dates (datetime types such as datetime64[ns]) and y column for numerical values\n",
    "        forecast : pandas DataFrame\n",
    "            The predicted result in a dataframe which was previously generated by\n",
    "            Prophet's model.predict(future)\n",
    "        beta : float\n",
    "            amplifying factor for setting lower and upperbounds for anomalies\n",
    "        test_window : int\n",
    "            Number of days for Prophet to make predictions for\n",
    "        Returns\n",
    "        -------\n",
    "        outliers : a list of (datetime, int, int) triple\n",
    "            A list of outliers, the date, the value, and penalty for each\n",
    "        df_pred : pandas DataFrame\n",
    "            The data set contains actual and predictions for the forecast time frame\n",
    "        P : int\n",
    "            Net penalty value of all the outliers detected\n",
    "        \"\"\"\n",
    "        df_pred = self.forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(self.test_window)\n",
    "        df_pred.index = df_pred['ds'].dt.to_pydatetime()\n",
    "        df_pred.columns = ['ds', 'preds', 'lower_y', 'upper_y']\n",
    "        end_index = self.data.df.index[-1]\n",
    "\n",
    "        test_start_index = end_index - self.test_window \n",
    "        df_pred['actual'] = self.data.df['y'][test_start_index:end_index].values\n",
    "\n",
    "        # construct a list of outliers\n",
    "        outlier_index = list()\n",
    "        outliers = list()\n",
    "        penalty = list()\n",
    "        P = 0 # net penalty\n",
    "        for i in range(df_pred.shape[0]):\n",
    "            actual_value = df_pred['actual'][i]\n",
    "            pred_value   = df_pred['preds'][i]\n",
    "            lower_bound  = (1-self.beta)*df_pred['lower_y'][i]\n",
    "            upper_bound  = (1+self.beta)*df_pred['upper_y'][i]\n",
    "            if actual_value < lower_bound:\n",
    "                outlier_index += [i]\n",
    "                p = (pred_value - actual_value)/pred_value\n",
    "                penalty.append(p)\n",
    "                outliers.append((df_pred.index[i-1], actual_value, p))\n",
    "                \n",
    "            elif actual_value > upper_bound:\n",
    "                outlier_index += [i]\n",
    "                p = (actual_value - pred_value)/pred_value\n",
    "                penalty.append(p)\n",
    "                outliers.append((df_pred.index[i-1], actual_value, p))            \n",
    "\n",
    "                # print out the evaluation for each outlier\n",
    "                print('=====')\n",
    "                print('actual value {} fall outside of the prediction interval'.format(actual_value))\n",
    "                print('interval: {} to {}'.format(lower_bound, upper_bound))\n",
    "                print('Date: {}'.format(str(df_pred.index[i])[:10]))\n",
    "\n",
    "        P = sum(penalty)\n",
    "        print('{}: Net Penalty for the prediction interval of last {} days is {}'.format(self.data.customer,self.test_window, P))\n",
    "        for outlier in outliers:\n",
    "            print(outlier)\n",
    "        \n",
    "        self.data.outliers = outliers #list\n",
    "        self.data.P = P  #scalar\n",
    "\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_plot(ad, color_cycle = [\"#4286f4\", \"#f44174\"], post_to_slack=False):#df, forecast, prophet_model, changepoints_list, outliers=list()):\n",
    "    \"\"\"\n",
    "    Plot the actual, predictions, and anomalous values\n",
    "    Args\n",
    "    ----\n",
    "    df : pandas DataFrame\n",
    "        The daily time-series data set contains ds column for\n",
    "        dates (datetime types such as datetime64[ns]) and y column for numerical values\n",
    "\n",
    "    outliers : a list of (datetime, int) tuple\n",
    "        The outliers we want to highlight on the plot.\n",
    "    \"\"\"\n",
    "    # generate the plot\n",
    "    fig = ad.model.plot(ad.forecast)\n",
    "    \n",
    "    # retrieve the subplot in the generated Prophets matplotlib figure\n",
    "    ax = fig.get_axes()[0]\n",
    "    ax.set_ylim(ymin=0)\n",
    "    #plot actual values\n",
    "    x_pydatetime = ad.data.df['ds'].dt.to_pydatetime()\n",
    "    ax.plot(x_pydatetime,\n",
    "        ad.data.df.y,\n",
    "        color='orange', label='Actual') \n",
    "    ax.set_title(f'{ad.data.customer}: Net Penalty for the prediction interval of last {ad.test_window} days is {ad.data.P}')\n",
    "\n",
    "    # plot each outlier in red, uncomment the second line to annotate date (makes it super crowded though)\n",
    "    for outlier in ad.data.outliers:\n",
    "        ax.scatter(outlier[0], outlier[1], s = 16, marker='x', color='red', label='Anomaly')\n",
    "        # ax.text(outlier[0], outlier[1], str(outlier[0])[:10], color='red')\n",
    "\n",
    "\n",
    "    # re-organize the legend\n",
    "    patch1 = mpatches.Patch(color='red', label='Anomaly')\n",
    "    patch2 = mpatches.Patch(color='orange', label='Actual')\n",
    "    patch3 = mpatches.Patch(color='skyblue', label='Prediction interval')\n",
    "    plt.legend(handles=[patch1, \n",
    "                        patch2, \n",
    "                        patch3, \n",
    "                        ])\n",
    "    \n",
    "    #plot the changepoints\n",
    "\n",
    "    color_cycle = cycle(color_cycle)\n",
    "    for (start, end), col in zip(pairwise(ad.data.changepoints), color_cycle):\n",
    "        ax.axvspan(ad.data.df.iloc[max(0,start-1)]['ds'], ad.data.df.iloc[end-1]['ds'], facecolor=col,alpha=0.2)\n",
    "\n",
    "    if post_to_slack:\n",
    "\n",
    "        plt.savefig('AD.jpg')\n",
    "        try:\n",
    "            response = client.files_upload(\n",
    "                file='AD.jpg',\n",
    "                title=f'{ad.data.customer} Penalty = {ad.data.P}',\n",
    "                channels='anomaly-detection'\n",
    "            )\n",
    "        except SlackApiError as e:\n",
    "            # You will get a SlackApiError if \"ok\" is False\n",
    "            assert e.response[\"ok\"] is False\n",
    "            assert e.response[\"error\"]  # str like 'invalid_auth', 'channel_not_found'\n",
    "            print(f\"Got an error: {e.response['error']}\")\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores.sort(lambda ad: ad.P)\n",
    "raw_data = DataGenerator(engine=engine, queries=queries, extract_window=60).run()\n",
    "ads = [AnomalyDetector(r, test_window=7, train_window=14, beta=0.1, \n",
    "    ruptures_changepnt_penalty=10, prophet_interval_width=0.95, \n",
    "    prophet_changepnt_prior = 0.15, weekly_seasonality= False).apply() for r in raw_data]\n",
    "ads_sorted = sorted(ads, key=lambda ad: ad.data.P, reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ad in ads_sorted:#[:20]:\n",
    "    prophet_plot(ad, post_to_slack=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('anomaly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e859af98375a5e4cccfb42649a6c309ca09ec633fd37c0d35138e8f12ba621a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
